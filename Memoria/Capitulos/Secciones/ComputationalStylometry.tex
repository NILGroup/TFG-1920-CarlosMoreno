\section{Computational stylometry}
This field of Artificial Intelligence (related with the Natural Language Processing and Natural Language Generation) is in charge of studying the writing style in natural language written documents (although it is often use in applications like the detection of plagiarism in programmes). In this section we are going to delve into it in order to known the state of art of this field of study. To achieve this, first a brief introduction is presented (see Section \ref{ssect:introstylo}) and, then, the different applications and techniques used in Computational stylometry are explained (see Section \ref{ssect:techstylo}).

In addition, it will be necessary to explain the presentation of computational stylometry in the specific field of e-mails (see Section \ref{ssect:styloemail}) since, as we can deduce, these present singularities with respect to other types of documents.

Finally, various style writing metrics are going to be explained (see Section \ref{ssect:stymet}) for the purpose of calculating and studying them in the extracted dataset (the entire set of emails that have been extracted).

\subsection{Introduction}\label{ssect:introstylo}
Stylometry \citep{wiki:stylometry} is the application of the study of linguistic style to written language, although it has also been successfully applied to music and painting. It could be defined as the linguistic discipline that applies statistical analysis to literature in order to evaluate the author's style through various quantitative criteria.

Stylometry is characterized by the assumption that there are implicit features in the texts that the author introduces unconsciously, such as the use of a specific vocabulary that makes up the writer's mental lexicon, the lexical-syntactic structure of the sentences in the document, etc \citep{burrows1992computers}.

According to \cite{stylohist}, the stylometry was born in 1851 when Augustus de Morgan, an English logician, hypothesized that the problem of authorship could be addressed by determining whether one text ``does not deal in longer words'' \citep{morganletters} than another. Following this idea, three decades later, the American physicist Thomas Mendenhall carried out research in which he measured the length of several hundred thousand words from the works of Bacon, Marlowe and Shakespeare \citep{mendenhall1887}. However its results showed that word length is not an effective writing style features which allow us to discriminate between different authors. Since then numerous investigations have been carried out to analyse the parameters that define writing style more precisely.

\cite{neuronalstylometry} defines the writing style as ``a set of measurable patterns which may be unique to an author''. For this reason, various machine learning and statistical techniques have been used to discover the characteristics that determine it. One of the first and most famous successes was the resolution of the controversial authorship of twelve of the Federalist Papers. These documents, a total of eighty-five papers, were published anonymously in 1787 to convince the citizens of New York State to ratify the constitution. They are known to have been written by Alexander Hamilton, John Jay and James Madison, who subsequently claimed their contributions from each of them. However, twelve were claimed both Madison and Hamilton. By using the frequency of occurrence of function words, previously used in \cite{juniusletters}, and employing numerical probabilities adjusted by Bayes' theorem, in \cite{federalistpapers} the twelve papers disputed were attributed to James Madison. Thereafter, Federalist Papers is a famous example in this area for testing the different solutions, as it happens in \cite{neuronalstylometry}, which make use of neural networks to solve this problem.

\subsection{Applications and techniques}\label{ssect:techstylo}
In addition to the detection and verification of authorship in historical, literary and even forensic investigations, stylometry is used in other areas such as the detection of fraud and plagiarism, the classification of documents according to their genre or audience, etc. Other possible applications of this area are the prediction of the gender, age or personality of the author as it happens in \cite{schwartz2013personality}; inference of the date of composition of texts, which is known as ``stylochronometry'' \citep{stamou2007stylochronometry, juola2007becoming}; and the natural language generation with Style \citep[Section 5.1]{nlgsoa}.

To address all these problems, mostly statistical techniques are used. Some of them, which are more complex, are more recognized for belonging to the field of machine learning such as neural networks \citep{ng1997feature}, Support Vector Machines \citep{abbasi2005applying}, Principal Components Analysis \citep{PCAstyle}, decision trees \citep{apte1998text}, Adaboost \citep{cheng2011author}, K-Nearest Neighbors \citep{kucukyilmaz2008chat} and Naive Bayes \citep{sahami1998bayesian}; while others are based on purely statistical approaches (such as cusum in \cite{summers1999analysing} or \cite{thisted1987did}) or merely syntactic-statistical concepts as in the well-known software implementations such as stylo \citep{stylor} and STYLENE \citep{stylene}. To this last type also belongs techniques based on dictionary word counting using Linguistic Inquiry and Word Count also known as LIWC \citep{liwc2015}, while more recent ones which use simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags \citep{mihalcea2009lie, ott2011finding}, belongs to the machine learning approach. We can also find techniques outside this paradigm, such as the writing style features driven from Context Free Grammar (CFG), as we can observe in \cite{cfgstylo}, genetic algorithms \citep{holmes1995federalist} and Markov chains \citep{tweedie1998variable}.

In order to address our work, we are going to make use of writing style metrics (which will be explained in Section \ref{ssect:stymet}), based on simple statistics like the mean and easy probabilistic metrics like the entropy.

\subsection{Style in emails}\label{ssect:styloemail}
Electronic mails are a very specific type of document in stylometry. Their length, usually shorter, and level of reliability, in most occasion between the informality of spoken word and the relative formality of an official letter, are two of their characteristic that make them so peculiar. For this reason, a lot of researches have focused their attention on these type of texts, taking special interest the identification pertaining to the authorship of e-mail messages such as the published thesis \cite{corney2003analysing} or \cite{thomson2001predicting}, which have investigated the existence of gender-preferential language styles in e-mail communications.

Despite being able to use most of the techniques mentioned above, both the machine learning (such as K-Nearest Neighbors used in \cite{calix2008stylometry} or Support Vector Machines used in \cite{de2001mining}) and the purely statistical approaches (such as regression algorithms used in \cite{iqbal2010mining} for analysing 292 different features in order to verify the email authorship), it is possible to find big differences with other documents such as structural features that pure text lacks. The usage of greeting text, farewell text and the inclusion of a signature are three examples of these structural features that we must to take into account.

Due to e-mail documents have several features which difference from with longer formal text documents (such as literary works or published articles), they make any computational stylometry problem challenging compared with others. First of all, as we have previously said, length of the emails is much shorter than other documents, which results in certain language-based metrics may not being appropriate (such as hapax legomena or hapax dislegomena, that is to say, the number or ratio of words used once or twice, respectively). This e-mail's feature also makes contents profiling based on traditional text document analysis techniques, such as the ``bag-of-words'' representation (for example when Naive Bayes approach is being used) more difficult.

Other electronic mail's particularity is the composition style used in formulating them. That is, an author profile derived from normal text documents (for example published articles) could not be the same as that obtained from a common e-mail document \citep{de2001mining}. For example, the briefness of the e-mails causes a greater tendency to get to the point without excessive detours on the subject, in other words, they have a concise nature. We may also find that they contain a greater number of grammatical errors or even a quick compositional style that is more similar to an oral interaction, as these can become a dialogue between two or more interlocutors. In this way, the authoring composition style and interactivity features attributed to electronic mails shares some elements of both formal writing and speech.

Main feature of e-mail against other type of documents that we are interested in is the variation in the individual style of e-mail messages due to the fact that they, as an informal and fast-paced medium, exhibit variations in an individual's writing styles due to the adaptation to distinct contexts or correspondents \citep{argamon2003style}. Many authors such as \cite{allen1974methods} and \cite{de2001mining} support the hypothesis that each writer has certain unconscious habits when writing an email that depend on the target audience. However, we hardly find any research that uses stylometry to set the parameters of writing style according to the recipient of the message.

\subsection{Style metrics}\label{ssect:stymet}

According to \cite{rudman1997state}, at least a thousand stylistic features have been proposed in stylometric research. However, there is no agreement among researchers regarding which ``style markers'' yield the best results. \cite{chen2011authorship} (150 stylistic features were extracted from e-mail messages for authorship verification), \cite{gruner2005tool} (sixty-two stylometric measurements applied to pairs of text were calculated and then analysing in order to detect plagiarism in text documents) and \cite{canales2011stylometry} (82 stylistic features extracted from sample exam documents were analysed using a K-Nearest Neighbours classifier for the purpose of authenticating online test takers) are only 3 examples of a large list of researches which look for appropriate writing style metrics to carry out their work.

As \cite{brocardo2013authorship} indicate, analysing a huge number of features does not necessarily provide the best results, as some features provide very little or no predictive information. And, as \cite{brocardo2013authorship} do, our approach is to build on previous works by identifying and keeping only the most discriminating features.

According to \cite{abbasi2008writeprints} existing stylistic features can be categorized as lexical, syntatic, structural, content-specific and idiosyncratic style markers. However, this is not the only existing classification. There are many others like the one proposed by \cite{corney2001identifying} (184 stylometric measurements were calculated and analysing by using a Support Vector Machine learning method in order to identify the authorship en electronic mails), in which we see how features are divided as character-based, word-based, document-based, function word frequency distribution and word length frequency distribution; or the one proposed by \cite{cfgstylo} which use a more simple classification of features in words, shallow syntax and deep syntax. As in our case we have used 31 lexical-syntactic features (due to previous studies, such as \cite{homem2011authorship}, yield encouraging results with lexical-syntactic features), following the classification of \cite{abbasi2008writeprints}, we will now divide them into 4 categories in which we have grouped them according to their usefulness in terms of what type of conclusions we can infer from each of them. These categories are: part of speech features (see Section \ref{sssect:posf}), punctuation features (see Section \ref{sssect:punctf}), vocabulary features (see Section \ref{sssect:vocabf}) and structural features (see Section \ref{sssect:strucf}). We must not confuse this latter category (which it belongs to the lexical features of the classification given in \cite{abbasi2008writeprints}) with the structural metrics explained in \cite{abbasi2008writeprints}.

Finally, in this Section, we are going to study other popular metrics which are not used in this work (see Section \ref{sssect:unusedf}). Some of then are going to belong to the structural, content-specific and idiosyncratic style markers of \cite{abbasi2008writeprints}, and the others are going to complete the categories to which the explained metrics belong (lexical and syntatic).

\subsubsection{Part of Speech features}\label{sssect:posf}

We will call our part of speech metrics as the syntactic features which have to do with the part of speech of each word of the e-mails. As have been used in many previous studies in stylometry, such as \cite{argamon1998style}, \cite{zhao2007searching}, \cite{ott2011finding} and \cite{cfgstylo}, we utilise part of speech (POS) tags to encode shallow syntactic information.

Following the suggestion of \cite{holmes1985analysis}, we count the number of nouns, verbs, adjectives, adverbs, pronouns, determinants, conjunctions and prepositions of each text. By calculating this, significant stylistic traits may be found, because as \cite{somers1966statistical} claims: ``A more cultivated intellectual habit of thinking can increase the number of substantives used, while a more dynamic empathy and attitude can be habitually expressed by means of an increased number of verbs. It is also possible to detect a number of idiosyncrasies in the use of prepositions, subordinations, conjunctions and articles''.

In adding to this metrics, we calculate the verb-adjective ratio, due to \cite{antosch1969diagnosis} obtained significant results by showing that this measure is dependent on the theme of the work, for example folk tales have high values and scientific works have low values.

Lastly, we calculate other style marker which is a proportion between certain classes: the determinant-pronouns ratio. In \cite{brainerd1974weighting}, there are evidence of a connection between the number of articles and the number of pronouns in a text.

\subsubsection{Punctuation features}\label{sssect:punctf}

As in \cite{baayen2002experiment} is studied, we try to extract conclusions from this syntactic features. In order to achieve this purpose, and following the example of \cite{calix2008stylometry}, we calculate the amount of commas, periods, semi-colons, ellipsis and pair of brackets. With these metrics we can reach conclusions such as the structural complexity of a message (since, for example, juxtaposition structures appear in the presence of some of these scores), the division into sentences of the message or the need for clarification of the text transmitted (for example, by analysing the amount of brackets).

\subsubsection{Vocabulary features}\label{sssect:vocabf}

Previous work, such as \cite{mihalcea2009lie} and \cite{ott2011finding}, has shown that ``bag of words'' are effective in detecting features in different documents. As \cite{allen1974methods}, claims: ``each writer tends to keep relatively constant the distribution of high frequency determiners, such as articles and conjunctions, whose information content is small compared to that of nouns and verbs. The other end of a frequency list is also of use in that sometimes a distinguishing stylistic feature is the avoidance of certain words''. In this way, we note how many times each different word is used in a message.

Of course the ``bag of words'' is not the only metric that we can categorise as a vocabulary feature and from which we can extract conclusions about the vocabulary used. There are many other which tries to set the parameters of, for instance, the difficult of the vocabulary or its richness.

As for the difficulty level, it determines the level of education that someone needs to have if they are to understand the text. There are several indices available to calculate this level, such as the proposed in \cite{dale1948formula}, the Gunning Fog Index \citep{wiki:gunning} or the Flesch-Kincaid index \citep{dubay2004principles}, although the latter is the most commonly documented and cited. The expression which determines the Flesch-Kincaid index is the following:

$$
I_{FK} = 1.599\lambda-1.015\beta-31.517
$$

Where $\lambda$ is the mean of one-syllable words per 100 words, and $\beta$ is the mean sentence length measured by the number of words. However, as we are not able to divide Spanish words by syllables, we determine $\lambda$ as the mean of words with two or less characters per 100 words.

In respect of

\subsubsection{Structural features}\label{sssect:strucf}

\subsubsection{Unused features}\label{sssect:unusedf}

In a vast majority of approaches, stylometrists rely on high-frequency items. Such features are typically extracted in level of (groups of) words, characters or part of speech, called n-grams \citep{kjell1994discrimination}. Whereas token level features have longer tradition in the field, character n-grams have been borrowed from the field of language identification in Computer Science \citep{stamatatos2009survey, eder2011style}. However, the most reliably successful features have been function words (short structure-determining words: common adverbs, auxiliary verbs, conjunctions, determiners, numbers, prepositions and pronouns) and word or part of speech n-grams. 

A number of successful experiments with function words have been reported, such as \cite{craig1999authorial}, \cite{koppel2006feature} and \cite{de2001mining}. N-grams (word or part of speech ones) to some extent overlap with function words, since frequent short words count higher, but their frequencies also take into account some punctuation and other structural properties of the text. Besides, due to n-gram features are noise tolerant and effective, and e-mails are non-structured documents, many researches about this specific type of texts, as \cite{brocardo2013authorship} and \cite{corney2001identifying}, have used them.

Most reports, such as the previously mentioned \cite{kjell1994discrimination} and \cite{corney2001identifying}, indicate that 2 or 3-grams gave good categorisation results for different text chunk sizes but these results were thought to be due to an inherent bias of some n-grams towards content rather than style alone. The effectiveness of n-grams comes from the fact that they are a successful summary marker, one that can substitute for other markers. It is able to capture characteristics about the author's favourite vocabulary, known as word n-grams \citep{diederich2003authorship}, as well as sentence structure, known as part of speech n-grams \citep{baayen1996outside, argamon1998routing}. The problem can be found with a small corpus, since, as \cite{baayen2000back} suggests, even successful style markers may not be representative for differentiating gender, theme, author, etc. in these cases.

Other metric based on the frequency of the items is the Probabilistic Context Free Grammar (PCFG) which is used by \cite{cfgstylo} in order to detect deception.

All the techniques for setting the parameters of writing style presented so far in this section have a higher level of complexity than others such as those mentioned in previous sections (like entropy in Section \ref{sssect:vocabf}). This may be due to a high level of memory required during calculations (as is the case with n-grams) or a higher algorithmic complexity (as in the case of PCFG). We can also find other simple popular metrics used in other research. A good example is the Burrow's Delta \citep{burrows2002delta}, which is an intuitive distance metric which has attracted a good share of attention in the community, also from a theoretical point of view \citep{argamon2008interpreting, hoover2004testing, hoover2004delta}. Another example is the type-token ratio, which is given by the formula $R=V/N$, where $N$ is the number of units (word occurrences) which form the sample text (tokens) and $V$ is the number of lexical units which form the vocabulary in the sample (types). The behaviour of this style marker was studied in \cite{kjetsaa1979and} and an approximation to Normal distribution of types per 500 tokens in all text analysed was found. Certainly it would seem that the type-token ratio would only be usful in comparative investigations where the value of $N$ is fixed.

As we have studied in Section \ref{sssect:MIMEheaders}, some e-mails use HTML formatting. With this information, \cite{de2001mining} includes the set of HTML tags as a structural metrics and studies the frequency distribution of them as one of their 21 structural attributes. These also include the number of attachments, position of requoted text within e-mail body, usage of greeting and/or farewell acknowledgement and the inclusion of a signature text. Other structural attributes, including technical features such as the use of various file extensions, fonts, sizes, and colours; have been used in researches as \cite{abbasi2005applying}.

In addition to the structural features, \cite{de2001mining} studies other lexical-syntactic features based on the amount of blank lines, the total number of lines, count of hapax legomena, the total number of alphabetic, upper-case and digit characters in words and the number of space, white-space and tab spaces in the text.

As for unused lexical-syntactic characteristics, we can also mention those defined in \cite{calix2008stylometry}, some of which are related to punctuation (such as based on the amount of dollar signs, ampersands, number signs, percent signs, apostrophes, asterisks, dashes, forward slashes, colons, pipe signs, mathematical signs, question and exclamation marks, at signs, backward slashes, caret signs, underscores, vertical lines, etc.), to sentence and paragraph (such as the number of sentences beginning with upper or lower case and the average number of words per paragraph) and to words (such as number of times ``well'' and ``anyhow'' appears). Other researches as \cite{corney2001identifying} make use of letter frequencies, distribution of syllables per word, hapax dislegomena, word collocations, preferred word positions, prepositional phrase structure and phrasal composition grammar. As regards frequency distributions of syllables per word, \cite{fucks1965mathematische} discovered that it discriminated different languages more than specific authors. However, in \cite{brainerd1974weighting}, it is claimed that a model based on a translated negative binomial distribution was a better fit to such distributions than \cite{fucks1965mathematische} translated Poisson distribution. Lastly, in \cite{brainerd1974weighting} concludes that some authors styles are more homogeneous than others as regards syllable count and it would appear that the distribution of syllables per word in a corpus, being an easily accessible index of its style, is one area that may prove profitable in stylometry studies.

Finally, in respect of idiosyncratic features, they include misspellings, grammatical mistakes, and other usage anomalies \citep{abbasi2008writeprints}. Such features are extracted using spelling and grammar checking tools and dictionaries \citep{chaski2001empirical}. Idiosyncrasies may also reflect deliberate author choices or cultural differences, such as use of the word ``center'' versus ``centre'' \citep{koppel2003exploiting}. Besides, we can add the study of features which determine the level of formality of the text, as it happens in \cite{sheika2012learning}.