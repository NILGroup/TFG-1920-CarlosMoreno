\section{Computational stylometry}\label{sect:compstyl}
This field of Artificial Intelligence (related with Natural Language Processing and Natural Language Generation) is in charge of studying the writing style in natural language written documents (although it is often used in applications like the detection of plagiarism in programmes). In this section we are going to delve into it in order to know the state of art of this field of study. To achieve this, first a brief introduction is presented (see Section \ref{ssect:introstylo}), and then the different applications and techniques used in Computational stylometry are explained (see Section \ref{ssect:techstylo}). In addition, it will be necessary to explain the presentation of computational stylometry in the specific field of e-mails (see Section \ref{ssect:styloemail}) since, as we can deduce, they present singularities with respect to other types of documents. Finally, various style writing metrics are going to be explained (see Section \ref{ssect:stymet}) for the purpose of calculating and studying them in the extracted dataset (the entire set of e-mails that have been extracted).

\subsection{Introduction to Computational Stylometry}\label{ssect:introstylo}
Stylometry \citep{hughes2012quantitative} is the application of the study of linguistic style to written language, although it has also been successfully applied to music (both in composition such as in the researches of \cite{manaris2005zipf}, \cite{casey2008analysis} and \cite{huron1991ramp}; and performances, such as the study carried out by \cite{sapp2008hybrid}) and visual arts \citep{taylor1999fractal, hughes2010quantification}. It could be defined as the linguistic discipline that applies statistical analysis to literature in order to evaluate the author's style through various quantitative criteria.

Stylometry is characterized by the assumption that there are implicit features in the texts that the author introduces unconsciously, such as the use of a specific vocabulary that makes up the writer's mental lexicon, the lexical-syntactic structure of the sentences in the document, etc \citep{burrows1992computers}.

According to \cite{stylohist}, stylometry was born in 1851 when Augustus de Morgan, an English logician, hypothesized that the problem of authorship could be addressed by determining whether one text ``does not deal in longer words'' \citep{morganletters} than another. Following this idea, three decades later, the American physicist Thomas Mendenhall carried out research in which he measured the length of several hundred thousand words from the works of Bacon, Marlowe and Shakespeare \citep{mendenhall1887}. However, its results showed that word length is not an effective writing style feature which allows us to discriminate between different authors. Since then, numerous investigations have been carried out to analyse the parameters that define writing style more precisely.

\cite{neuronalstylometry} define the writing style as ``a set of measurable patterns which may be unique to an author''. For this reason, various machine learning and statistical techniques have been used to discover the characteristics that determine it. One of the first and most famous successes was the resolution of the controversial authorship of twelve of the Federalist Papers. These documents, a total of eighty-five papers, were published anonymously in 1787 to convince the citizens of New York State to ratify the constitution. They are known to have been written by Alexander Hamilton, John Jay and James Madison, who subsequently claimed their contributions from each of them. However, twelve were claimed by both Madison and Hamilton. By using the frequency of occurrence of function words, previously used by \cite{juniusletters}, and employing numerical probabilities adjusted by Bayes' theorem, \cite{federalistpapers} attribute the twelve papers disputed to James Madison. Thereafter, Federalist Papers is a famous example in this area for testing the different solutions, for example \cite{neuronalstylometry} make use of neural networks to solve this problem.

\subsection{Applications and techniques}\label{ssect:techstylo}
In addition to the detection and verification of authorship in historical, literary and even forensic investigations, stylometry is used in other areas such as the detection of fraud and plagiarism, the classification of documents according to their genre or audience, etc. Other possible applications of this area are the prediction of the gender, age or personality of the author as \cite{schwartz2013personality} studied; inference of the date of composition of texts, which is known as ``stylochronometry'' \citep{stamou2007stylochronometry, juola2007becoming}; and even natural language generation with style \citep[Section 5.1]{nlgsoa}.

To address all these problems, statistical techniques are mostly used. Some of them belong to the field of machine learning such as Neural Networks \citep{ng1997feature}, Support Vector Machines \citep{abbasi2005applying}, Principal Components Analysis \citep{PCAstyle}, Decision Trees \citep{apte1998text}, Adaboost \citep{cheng2011author}, K-Nearest Neighbors \citep{kucukyilmaz2008chat} and Naive Bayes \citep{sahami1998bayesian}. Others are based on purely statistical approaches (such as cusum in \cite{summers1999analysing} or \cite{thisted1987did}) or merely syntactic-statistical concepts as in the well-known software implementations such as stylo \citep{stylor} and STYLENE \citep{stylene}. To this last type also belong techniques based on dictionary word counting using Linguistic Inquiry and Word Count also known as LIWC \citep{liwc2015}, while more recent ones which use simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags \citep{mihalcea2009lie, ott2011finding}, belong to the machine learning approach. We can also find techniques outside this paradigm, such as the writing style features driven from Context Free Grammar (CFG), as we can observe in the research of \cite{cfgstylo}, genetic algorithms \citep{holmes1995federalist} and Markov chains \citep{tweedie1998variable}.

\subsection{Style in e-mails}\label{ssect:styloemail}
Electronic mails are a very specific type of document in stylometry. Their length, usually quite short, and the level of reliability, in most occasions between the informality of spoken word and the relative formality of an official letter, are two of their characteristic that make them so peculiar. For this reason, a lot of researchers have focused their attention on these type of texts, taking special interest the identification pertaining to the authorship of e-mail messages such as the published thesis by \cite{corney2003analysing} or \cite{thomson2001predicting}, which have investigated the existence of gender-preferential language styles in e-mail communications.

Despite being able to use most of the techniques mentioned above, both the machine learning (such as K-Nearest Neighbors used by \cite{calix2008stylometry} or Support Vector Machines used by \cite{de2001mining}) and the purely statistical approaches (such as regression algorithms used by \cite{iqbal2010mining} for analysing 292 different features in order to verify the e-mail authorship), it is possible to find big differences with other documents such as structural features that pure text lacks. The usage of greeting text, farewell text and the inclusion of a signature are three examples of these structural features that we must take into account.

Due to that e-mail documents have several features which distinguish them from longer formal text documents (such as literary works or published articles), they make any computational stylometry problem challenging compared with others. First of all, as we have previously said, the length of the e-mails is much shorter than other documents, which results in certain language-based metrics not being appropriate (such as \textit{hapax legomena} or \textit{hapax dislegomena}, that is to say, the number or ratio of words used once or twice, respectively). This e-mail's feature also makes contents profiling based on traditional text document analysis techniques, such as the ``bag-of-words'' representation (for example when Naive Bayes approach is being used) more difficult.

Other electronic mail's particularity is the composition style used in formulating them. That is, an author profile derived from normal text documents (for example published articles) can not be the same as that obtained from a common e-mail document \citep{de2001mining}. For example, the briefness of the e-mails causes a greater tendency to get to the point without excessive detours on the subject, in other words, they have a concise nature. We may also find that they contain a greater number of grammatical errors or even a quick compositional style that is more similar to an oral interaction, as these can become a dialogue between two or more interlocutors. In this way, the authoring composition style and interactivity features attributed to electronic mails shares some elements of both formal writing and speech.

The main feature of e-mail against other types of documents that we are interested in is the variation in the individual style of e-mail messages due to the fact that they, as an informal and fast-paced medium, exhibit variations in an individual's writing styles due to the adaptation to distinct contexts or correspondents \citep{argamon2003style}. Many authors such as \cite{allen1974methods} and \cite{de2001mining} support the hypothesis that each writer has certain unconscious habits when writing an e-mail that depend on the target audience. However, to the best of our knowledge, there is no research that uses stylometry to set the parameters of writing style according to the recipient of the message.

\subsection{Style metrics}\label{ssect:stymet}

According to \cite{rudman1997state}, at least a thousand stylistic features have been proposed in stylometric research. However, there is no agreement among researchers regarding which ``style markers'' yield the best results. \cite{chen2011authorship} (150 stylistic features were extracted from e-mail messages for authorship verification), \cite{gruner2005tool} (sixty-two stylometric measurements applied to pairs of text were calculated and then analysed in order to detect plagiarism in text documents) and \cite{canales2011stylometry} (82 stylistic features extracted from sample exam documents were analysed using a K-Nearest Neighbours classifier for the purpose of authenticating online test takers) are only three examples of a large list of researches which look for appropriate writing style metrics to carry out their work.

As \cite{brocardo2013authorship} indicate, analysing a huge number of features does not necessarily provide the best results, as some features provide very little or no predictive information. And, as \cite{brocardo2013authorship} do, our approach is to build on previous works by identifying and keeping only the most discriminating features.

According to \cite{abbasi2008writeprints} existing stylistic features can be categorised as lexical (word, or character-based statistical measures of lexical variation), syntactic (including function words, punctuation and part-of-speech tag n-grams), structural (especially useful for online text, include attributes relating to text organization and layout), content-specific (are comprised of important keywords and phrases on certain topics) and idiosyncratic (include misspellings, grammatical mistakes, and other usage anomalies) style markers. However, this is not the only existing classification. There are many others like the one proposed by \cite{corney2001identifying}, in which we see how features are divided as character-based, word-based, document-based, function word frequency distribution and word length frequency distribution; or the one proposed by \cite{cfgstylo} which use a more simple classification of features in words, shallow syntax and deep syntax. However, hereafter, we are going to use the classification explained in \cite{abbasi2008writeprints} for referring to the different metrics.

In a vast majority of approaches, stylometrists rely on high-frequency items. Such features are typically extracted in level of (groups of) words, characters or part of speech, called n-grams \citep{kjell1994discrimination}. Whereas token level features have longer tradition in the field, character n-grams have been borrowed from the field of language identification in Computer Science \citep{stamatatos2009survey, eder2011style}. However, the most reliably successful features have been function words (short structure-determining words: common adverbs, auxiliary verbs, conjunctions, determiners, numbers, prepositions and pronouns) and word or part of speech n-grams. 

A number of successful experiments with function words have been reported, such as \cite{craig1999authorial}, \cite{koppel2006feature} and \cite{de2001mining}. N-grams (word or part of speech ones) to some extent overlap with function words, since frequent short words count higher, but their frequencies also take into account some punctuation and other structural properties of the text. Besides, due to n-gram features are noise tolerant and effective, and e-mails are non-structured documents, many researches working with this specific type of texts, as \cite{brocardo2013authorship} and \cite{corney2001identifying}, have used them.

Most reports, such as the previously mentioned composed by \cite{kjell1994discrimination} and \cite{corney2001identifying}, indicate that 2 or 3-grams gave good categorisation results for different text chunk sizes but these results were thought to be due to an inherent bias of some n-grams towards content rather than style alone. The effectiveness of n-grams comes from the fact that they are a successful summary marker, which can replace other markers. It is able to capture characteristics about the author's favourite vocabulary, known as word n-grams \citep{diederich2003authorship} and are a content-specific feature, as well as sentence structure, known as part of speech n-grams \citep{baayen1996outside, argamon1998routing}, which are a syntactic feature. The problem can be found with a small corpus, since, as \cite{baayen2000back} suggest, even successful style markers may not be representative for differentiating gender, theme, author, etc. in these cases.

Another metric based on the frequency of the items is the Probabilistic Context Free Grammar (PCFG) which is used by \cite{cfgstylo} in order to detect deception.

All the techniques for setting the parameters of writing style presented so far in this section have a higher level of complexity than others. This may be due to a high level of memory required during calculations (as is the case with n-grams) or a higher algorithmic complexity (as in the case of PCFG). We can also find other simple popular metrics used in other research. A good example is the lexical feature of Burrow's Delta \citep{burrows2002delta}, which is an intuitive distance metric which has attracted a good share of attention in the community, also from a theoretical point of view \citep{argamon2008interpreting, hoover2004testing, hoover2004delta}. Another example is the lexical feature of type-token ratio, which is given by the formula $R=V/N$, where $N$ is the number of units (word occurrences) which form the sample text (tokens) and $V$ is the number of lexical units which form the vocabulary in the sample (types). The behaviour of this style marker was studied in \cite{kjetsaa1979and} and an approximation to Normal distribution of types per 500 tokens in all text analysed was found. Certainly it would seem that the type-token ratio would only be useful in comparative investigations where the value of $N$ is fixed.

In order to study the sentence structure, as part of speech n-grams do, there are many other style markers such as the syntactic feature given by calculating the percentage of part of speech (POS) tags (which have been used in many previous studies in stylometry, such as \cite{argamon1998style}, \cite{zhao2007searching}, \cite{ott2011finding} and \cite{cfgstylo}) and the proportion of stop words in a text proposed in \cite{ril2014determination}. One possible approach consists in the style features which take into account the part of speech tags. The verb-adjective ratio and the article-pronoun ratio belong to this category. The first was proposed in \cite{antosch1969diagnosis} and significant results were obtained by showing that this measure is dependent on the theme of the work. For example, folk tales have higher values and scientific works have lower values. The second was studied by \cite{brainerd1974weighting}, where there is evidence of a connection between the number of articles and the number of pronouns in a text.

It is also possible to extract conclusions about the sentence structure through the punctuation features (which belong to the syntactic metrics category), as \cite{baayen2002experiment} studied. One possibility of metrics is to calculate the amount of commas, periods, semi-colons, ellipsis and brackets as \cite{calix2008stylometry} did.

As we have studied in Section \ref{sssect:MIMEheaders}, some e-mails use HTML formatting. With this information, \cite{de2001mining} includes the set of HTML tags as a structural metrics and studies the frequency distribution of them as one of their 21 structural attributes. These also include the number of attachments, position of requoted text within e-mail body, usage of greeting and/or farewell acknowledgement and the inclusion of a signature text. Other structural attributes, including technical features such as the use of various file extensions, fonts, sizes, and colours, have been used in works such as \cite{abbasi2005applying}. This is another possibility for studying the sentence structure with an structural feature approach. 

In addition to the structural features, \cite{de2001mining} study other lexical-syntactic metrics based on the amount of blank lines, the total number of lines, count of hapax legomena, the total number of alphabetic, upper-case and digit characters in words and the number of space, white-space and tab spaces in the text.

As for the lexical-syntactic characteristics, we can also mention those defined in \cite{calix2008stylometry}, some of which are related to punctuation (such as based on the amount of dollar signs, ampersands, number signs, percent signs, apostrophes, asterisks, dashes, forward slashes, colons, pipe signs, mathematical signs, question and exclamation marks, at signs, backward slashes, caret signs, underscores, vertical lines, etc.), to sentence and paragraph (such as the number of sentences beginning with upper or lower case and the average number of words per paragraph) and to words (such as number of times ``well'' and ``anyhow'' appear). Other researchers such as \cite{corney2001identifying} (184 stylometric measurements were calculated and analysed by using a Support Vector Machine learning method in order to identify the authorship of electronic mails) make use of letter frequencies, distribution of syllables per word, hapax dislegomena, word collocations, preferred word positions, prepositional phrase structure and phrasal composition grammar. As regards frequency distributions of syllables per word, \cite{fucks1965mathematische} discovered that it discriminated different languages more than specific authors. However, in \cite{brainerd1974weighting}, it is claimed that a model based on a translated negative binomial distribution was a better fit to such distributions than \cite{fucks1965mathematische} translated Poisson distribution. Lastly, \cite{brainerd1974weighting} concludes that some authors styles are more homogeneous than others with regard to syllable count and it would appear that the distribution of syllables per word in a corpus, being an easily accessible index of its style, is one area that may prove profitable in stylometry studies.

The most famous and ancient (as we have seen in Section \ref{ssect:introstylo}) lexical feature is the word length (it is also applied to each part of speech as it is explained by \cite{allen1974methods}). However, as \cite{smith1983recent} concludes: ``Mendenhall's method now appears to be so unreliable that any serious student of authorship should discard it''. Besides, it is too strongly influenced by the language used or the subject matter dealt with and, furthermore, cannot always admit enough variance to be significant. A better way to measure style based on this criterion is to construct a graph to show what percentage of words in the text have one letter, two letters, three, and so on up to the length of the longest word; but the influence of the language itself on such measurements cannot be denied \citep{williams1970style}.

A variation of the word length is the sentence length. It was proposed in \cite{yule1939sentence} and its major advantage is that there is a much wider range of words per sentence than letters per word. However, the major disadvantage is that it can be easily controlled by an author and it requires more text than is needed for measuring average word lengths.

Other very popular lexical features are those which measure the diversity of a text (such as the Simpson's Index, presented in \cite{simpson1949measurement}, or entropy, used in \cite{holmes1985analysis}), the richness of its vocabulary (such as the Yule's Characteristic, defined in \cite{yule2014statistical}, and the definition of richness proposed by \cite{honore1979some}) and the level of difficulty, such as the proposed in \cite{dale1948formula}, the Gunning Fog Index \citep{wiki:gunning} or the Flesch-Kincaid index \citep{dubay2004principles}.

As for the content-specific features, the most popular metric, a part from the word n-gram, is known as the ``bag of words'', which consists of storing how many times each word appears. Previous work, such as \cite{mihalcea2009lie} and \cite{ott2011finding}, has shown that ``bag of words'' are effective in detecting features in different documents. As \cite{allen1974methods} claims: ``each writer tends to keep relatively constant the distribution of high frequency determiners, such as articles and conjunctions, whose information content is small compared to that of nouns and verbs. The other end of a frequency list is also of use in that sometimes a distinguishing stylistic feature is the avoidance of certain words''.

Finally, in respect of idiosyncratic features, they include misspellings, grammatical mistakes, and other usage anomalies \citep{abbasi2008writeprints}. Such features are extracted using spelling and grammar checking tools and dictionaries \citep{chaski2001empirical}. Idiosyncrasies may also reflect deliberate author choices or cultural differences, such as use of the word ``center'' versus ``centre'' \citep{koppel2003exploiting}. Besides, we can add the study of features which determine the level of formality of the text, as it happens in \cite{sheika2012learning}.