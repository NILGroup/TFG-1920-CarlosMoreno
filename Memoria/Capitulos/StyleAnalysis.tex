\chapter{Style feature analysis}\label{cap:analysis}

\chapterquote{``Data! data! data!'' he cried impatiently. ``I can't make bricks without clay.''}{The adventure of the Copper Beeches\\Arthur Conan Doyle (1892)}

Once we have calculated the values of 31 metrics of all the messages of a user, our goal is to determine which ones vary and differ depending on the recipient of the message, that is to say, which one distinguish better the different receivers of the information. With this in mind, in this chapter we analyse the resulting values of measuring each message with the style analyser.

The first step in our analysis is to prepare the data. For this purpose, we have to categorise the different contacts depending on their relationship with the sender, choose the style metrics that we want to study, and modify their values in order to have the data ready for being analysed (with techniques such as standardisation and normalisation). This data preparation process is presented in Section \ref{sect:DatPrep}.

With a defined classification of each contact, we can carry out a preliminary analysis of the style descriptors considered using clustering techniques (see Section \ref{sect:clust1}). With this analysis we want to evaluate how well metrics fit to our categorisation. Thus, we can value the effectiveness of our chosen style metrics in order to distinguish the writing style based on the recipient of the message.

As we will see, due to the big amount of style metrics, to describe the features and the different categories of contacts is too difficult. For this reason, a dimension reduction is required. In this way, we can focus on the most significant dimensions which distinguish between the e-mails sent to different recipients. There is a wide variety of dimension reduction techniques, but we will start with the most popular of them: Principal Component Analysis (see Section \ref{sect:pca}). With this method, due to its nature, we can obtain results with as many features as we want, but with the disadvantage that PCA does not take into account our categories. In addition to it, each category is not well balanced. For these reasons, it is also necessary to use a less common dimension reduction technique: Gini Importance of Decision Trees (see Section \ref{sect:dectrees}). With the application of this machine learning method, wa can take into account our classification and reduce the system to only eight dimensions.

Finally, we repeat the analysis with clustering techniques, but this time with the obtained eight dimensions (see Section \ref{sect:clust2}). The purpose of this is to value the effectiveness of these metrics in the task of distinguishing between messages with different recipients.

\section{Data preparation: e-mail classification, metrics choice and correlation analysis}\label{sect:DatPrep}
\input{Capitulos/Secciones/DataPreparing}

\section{Preliminary analysis of the metrics considered using clustering techniques}\label{sect:clust1}
\input{Capitulos/Secciones/Clustering}

\section{Dimension reduction using Principal Component Analysis}\label{sect:pca}
\input{Capitulos/Secciones/PCA}

\section{Dimension reduction using Decision Trees}\label{sect:dectrees}
\input{Capitulos/Secciones/DecisionTrees}

\section{Analysis of the chosen metrics using clustering techniques}\label{sect:clust2}
\input{Capitulos/Secciones/Clustering2}

\section{Conclusions}
After establishing twelve categories, which classify all the recipients, and analysing the correlation between each possible pair of metrics (which generally are not correlated), we apply clustering techniques and use Adjusted Rand Index to evaluate how well metrics fit to our categorisation. Nevertheless, the results show us that our classification does not match with the obtained by K-Means and DBSCAN algorithms. In addition to it, as the number of metrics is too big, we can not easily describe them. For these reasons we make use of the dimension reduction techniques.

Despite being the most popular dimension reduction technique, Principal Component Analysis has the problem that it does not take into account our classification, which is an important fact because as the twelve categories are very unbalanced, the lack of a little percentage of explained variance ratio may mean that we are not taking into account an entire class. Therefore, we must use a less popular method: Gini Importance of Decision Trees. With this technique we are able to reduce the system to eight metrics safely guaranteeing that the removed style markers are much less important than the chosen features. In spite of obtaining again bad results in comparison with the classification given by K-Means and DBSCAN, these reduction in the number of metrics allow us to design a model which takes advantage of them in order to generate e-mails based on their recipients.